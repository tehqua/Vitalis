{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114ac571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff17596",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_path = \"./google_medgemma_4b_it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5158da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 883/883 [00:00<00:00, 891.58it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                       \n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=downloaded_path,   \n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39775a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=100) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient has rash on arm, itchy for 3 days. Assess severity:\n",
      "\n",
      "*   **Mild:** Few small bumps, localized to one area, not interfering with daily activities.\n",
      "*   **Moderate:** More widespread rash, some bumps, some itching, interfering with daily activities slightly.\n",
      "*   **Severe:** Extensive rash, many bumps, intense itching, interfering with sleep and daily activities.\n",
      "\n",
      "Patient reports: \"It's just a few small red bumps on my arm, and it's itchy, but I can still get dressed and do my work.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Patient has rash on arm, itchy for 3 days. Assess severity:\"\n",
    "\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9be0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 131/131 [00:00<00:00, 1078.54it/s, Materializing param=shared.weight]                                                      \n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'ExaoneMoeForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'YoutuForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "corrector = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"t5-small\",  # Nhẹ hơn MedGemma 60+ lần\n",
    "    device=\"cuda\"\n",
    ")\n",
    "text = \"Patient has acute pe in right lung\"\n",
    "\n",
    "result = corrector(\n",
    "    f\"\"\"You are an AI assistant specialized in processing medical audio transcripts.\n",
    "The input is raw text generated from speech-to-text transcription and may contain recognition errors, filler words, repetitions, broken sentences, incorrect grammar, or misspelled medical terms.\n",
    "\n",
    "Your task is to clean, correct, and edit the transcript while strictly preserving the original medical meaning.\n",
    "\n",
    "Instructions:\n",
    "- Correct spelling, grammar, and speech-to-text errors.\n",
    "- Remove filler words (e.g., \"uh\", \"um\", \"you know\") and unnecessary repetitions if they do not carry medical meaning.\n",
    "- Combine fragmented or incomplete sentences into clear, coherent sentences.\n",
    "- Standardize medical terminology using commonly accepted clinical terms.\n",
    "- Do NOT infer, diagnose, interpret, or add any new medical information.\n",
    "- Do NOT change clinical intent, timing, or certainty expressed in the original transcript.\n",
    "- If any segment is unclear, incomplete, or unintelligible, retain it and mark it as [inaudible] or [unclear]. Do NOT guess.\n",
    "\n",
    "Output requirements:\n",
    "- Provide only the edited transcript.\n",
    "- Do not include explanations, summaries, or comments.\n",
    "- Maintain a neutral, professional clinical tone.\n",
    "- Preserve speaker intent and context (e.g., physician–patient conversation).\n",
    "\n",
    "Here is the raw transcript:\n",
    "{{PASTE_RAW_TRANSCRIPT_HERE}}\n",
    "\"\"\",\n",
    "    max_length=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9819faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'correct grammar: Patient has acute pe in right lung'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
